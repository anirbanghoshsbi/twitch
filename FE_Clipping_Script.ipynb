{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Exposure clipping by gradient descent\n",
    "\n",
    "This notebook is a tensorflow port of [@MDO](https://numer.ai/mdo)'s [pytorch code](https://forum.numer.ai/t/model-diagnostics-feature-exposure/899/12) by [@jrb](https://numer.ai/jrb).\n",
    "\n",
    "Maintainer: [@arbitrage](https://numer.ai/arbitrage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "NUMERAI_S3_BUCKET_URL = \"https://numerai-public-datasets.s3-us-west-2.amazonaws.com\"\n",
    "\n",
    "#read in the example predictions from local storage\n",
    "#EXAMPLE_PREDS = 'tournament_predictions.csv'\n",
    "\n",
    "#or downlod the example predictions from Numerai's S3 bucket:\n",
    "EXAMPLE_PREDS_URL = NUMERAI_S3_BUCKET_URL + \"/latest_numerai_example_predictions_data.csv.xz\"\n",
    "\n",
    "#download the latest tournament data file:\n",
    "TOURNAMENT_DATA_URL = NUMERAI_S3_BUCKET_URL + \"/latest_numerai_tournament_data.csv.xz\"\n",
    "\n",
    "###IMPORTANT! DELETE THE FILE BELOW IF YOU CHANGE MODELS! OTHERWISE, RENAME THE FILE FOR YOUR VARIOUS MODELS###\n",
    "LM_CACHE_FILE = pathlib.Path(\"neutralization.cache.joblib\")\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True, experimental_compile=True)\n",
    "def exposures(x, y):\n",
    "    x = x - tf.math.reduce_mean(x, axis=0)\n",
    "    x = x / tf.norm(x, axis=0)\n",
    "    y = y - tf.math.reduce_mean(y, axis=0)\n",
    "    y = y / tf.norm(y, axis=0)\n",
    "    return tf.matmul(x, y, transpose_a=True)\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_loop_body(model, feats, pred, target_exps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        exps = exposures(feats, pred[:, None] - model(feats, training=True))\n",
    "        loss = tf.reduce_sum(tf.nn.relu(tf.nn.relu(exps) - tf.nn.relu(target_exps)) +\n",
    "                             tf.nn.relu(tf.nn.relu(-exps) - tf.nn.relu(-target_exps)))\n",
    "    return loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "def train_loop(model, optimizer, feats, pred, target_exps, era):\n",
    "    for i in range(1000000):\n",
    "        loss, grads = train_loop_body(model, feats, pred, target_exps)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        if loss < 1e-7:\n",
    "            break\n",
    "        if i % 10000 == 0:\n",
    "            tqdm.write(f'era: {era[3:]} loss: {loss:0.7f}', end='\\r')\n",
    "            \n",
    "def reduce_exposure(prediction, features, max_exp, era, weights=None):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(310),\n",
    "        tf.keras.experimental.LinearModel(use_bias=False),\n",
    "    ])\n",
    "    feats = tf.convert_to_tensor(features - 0.5, dtype=tf.float32)\n",
    "    pred = tf.convert_to_tensor(prediction, dtype=tf.float32)\n",
    "    if weights is None:\n",
    "        optimizer = tf.keras.optimizers.Adamax()\n",
    "        start_exp = exposures(feats, pred[:, None])\n",
    "        target_exps = tf.clip_by_value(start_exp, -max_exp, max_exp)\n",
    "        train_loop(model, optimizer, feats, pred, target_exps, era)\n",
    "    else:\n",
    "        model.set_weights(weights)\n",
    "    return pred[:,None] - model(feats), model.get_weights()\n",
    "\n",
    "def reduce_all_exposures(df, column=[\"prediction\"], neutralizers=None,\n",
    "                                     normalize=True,\n",
    "                                     gaussianize=True,\n",
    "                                     era_col=\"era\",\n",
    "                                     max_exp=0.1): ###<-----SELECT YOUR MAXIMUM FEATURE EXPOSURE HERE###\n",
    "    if neutralizers is None:\n",
    "        neutralizers = [x for x in df.columns if x.startswith(\"feature\")]\n",
    "    neutralized = []\n",
    "    if LM_CACHE_FILE.is_file():\n",
    "        cache = joblib.load(LM_CACHE_FILE)\n",
    "        # Remove weights for eraX if we'd accidentally saved it in the past.\n",
    "        cache.pop(\"eraX\", None)\n",
    "    else:\n",
    "        cache = {}\n",
    "    for era in tqdm(df[era_col].unique()):\n",
    "        tqdm.write(era, end='\\r')\n",
    "        df_era = df[df[era_col] == era]\n",
    "        scores = df_era[column].values\n",
    "        exposure_values = df_era[neutralizers].values\n",
    "\n",
    "        if normalize:\n",
    "            scores2 = []\n",
    "            for x in scores.T:\n",
    "                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n",
    "                if gaussianize:\n",
    "                    x = scipy.stats.norm.ppf(x)\n",
    "                scores2.append(x)\n",
    "            scores = np.array(scores2)[0]\n",
    "\n",
    "        scores, weights = reduce_exposure(scores, exposure_values,\n",
    "                                          max_exp, era, cache.get(era))\n",
    "        if era not in cache and era != \"eraX\":\n",
    "            cache[era] = weights\n",
    "            joblib.dump(cache, LM_CACHE_FILE)\n",
    "        scores /= tf.math.reduce_std(scores)\n",
    "        scores -= tf.reduce_min(scores)\n",
    "        scores /= tf.reduce_max(scores)\n",
    "        neutralized.append(scores.numpy())\n",
    "\n",
    "    predictions = pd.DataFrame(np.concatenate(neutralized),\n",
    "                               columns=column, index=df.index)\n",
    "    return predictions\n",
    "\n",
    "#If CUDA isn't set up properly for Tensorflow, then at least maximize the number of threads available for CPU\n",
    "if not tf.config.list_physical_devices('GPU'):  # No GPU(s) found\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(os.cpu_count() // 2)\n",
    "\n",
    "#read-in or download the example predictions\n",
    "exp_df = pd.read_csv(EXAMPLE_PREDS_URL, index_col=0)\n",
    "\n",
    "#download the tournament data\n",
    "tournament_df = pd.read_csv(TOURNAMENT_DATA_URL, index_col=0)\n",
    "\n",
    "#merge them together\n",
    "full_df = pd.merge(tournament_df, exp_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell executes the full script above and neutralizes the predictions to achieve a maximum 0.1 Feature Exposure\n",
    "neutralized_df = reduce_all_exposures(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your prediction file locally\n",
    "neutralized_df.to_csv(\"neutralized_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
